{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lesson_applied_modeling_1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KMI2k-oBsS08"
      },
      "source": [
        "Lambda School Data Science, Unit 2: Predictive Modeling\n",
        "\n",
        "# Applied Modeling, Module 1\n",
        "\n",
        "#### Objectives\n",
        "- define the problem at hand and the data on which you’ll train\n",
        "- choose how you’ll measure success on your problem\n",
        "- get and interpret the confusion matrix for classification models\n",
        "- use classification metrics: precision, recall\n",
        "- understand the relationships between precision, recall, thresholds, and predicted probabilities\n",
        "- understand how Precision@K can help make decisions and allocate budgets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUFLfGEuMucH",
        "colab_type": "text"
      },
      "source": [
        "### Setup\n",
        "\n",
        "#### If you're using [Anaconda](https://www.anaconda.com/distribution/) locally\n",
        "\n",
        "Install required Python package, if you haven't already:\n",
        "\n",
        "[category_encoders](http://contrib.scikit-learn.org/categorical-encoding/), version >= 2.0  \n",
        "\n",
        "`conda install -c conda-forge category_encoders`\n",
        "\n",
        "or\n",
        "\n",
        "`pip install --upgrade category_encoders`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HHfMGCjMvIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If you're in Colab...\n",
        "import os, sys\n",
        "in_colab = 'google.colab' in sys.modules\n",
        "\n",
        "if in_colab:\n",
        "    # Install required python package:\n",
        "    # category_encoders, version >= 2.0\n",
        "    !pip install --upgrade category_encoders\n",
        "    \n",
        "    # Pull files from Github repo\n",
        "    os.chdir('/content')\n",
        "    !git init .\n",
        "    !git remote add origin https://github.com/LambdaSchool/DS-Unit-2-Applied-Modeling.git\n",
        "    !git pull origin master\n",
        "    \n",
        "    # Change into directory for module\n",
        "    os.chdir('module1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40FPLCKQn_iJ",
        "colab_type": "text"
      },
      "source": [
        "#### Don't use Matplotlib version 3.1.1\n",
        "\n",
        "Why? Because of this issue: [sns.heatmap top and bottom boxes are cut off](https://github.com/mwaskom/seaborn/issues/1773)\n",
        "\n",
        "> This was a matplotlib regression introduced in 3.1.1 which has been fixed in 3.1.2 (still forthcoming). For now the fix is to downgrade matplotlib to a prior version.\n",
        "\n",
        "If you the following cell throws an `AssertionError`, then you have matplotlib version 3.1.1, and you may want to downgrade to version 3.1.0 to run this notebook: `pip install matplotlib==3.1.0`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpFoag9QoTgA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from distutils.version import StrictVersion\n",
        "import matplotlib\n",
        "version = matplotlib.__version__\n",
        "print('Matplotlib version', version)\n",
        "assert StrictVersion(version) != StrictVersion('3.1.0')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHsrndB4UY7Z",
        "colab_type": "text"
      },
      "source": [
        "## Process for Data Science\n",
        "\n",
        "#### Renee Teate, [Becoming a Data Scientist, PyData DC 2016 Talk](https://www.becomingadatascientist.com/2016/10/11/pydata-dc-2016-talk/)\n",
        "\n",
        "![](https://image.slidesharecdn.com/becomingadatascientistadvice-pydatadc-shared-161012184823/95/becoming-a-data-scientist-advice-from-my-podcast-guests-55-638.jpg?cb=1476298295)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ql_47slLVAer",
        "colab_type": "text"
      },
      "source": [
        "#### _This diagram is general and high-level. How do we apply it when doing predictive modeling with labeled, tabular data?_\n",
        "\n",
        "Business Question ➡ Data Question = steps 1-3 below\n",
        "\n",
        "Data Question ➡ Data Answer = steps 4-6 below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3TLAoRJUmTn",
        "colab_type": "text"
      },
      "source": [
        "## Process for Supervised Learning\n",
        "\n",
        "#### Francois Chollet, [Deep Learning with Python](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/README.md), Chapter 4: Fundamentals of machine learning, \"A universal workflow of machine learning\"\n",
        " \n",
        "> **1. Define the problem at hand and the data on which you’ll train.** Collect this data, or annotate it with labels if need be.\n",
        "\n",
        "> **2. Choose how you’ll measure success on your problem.** Which metrics will you monitor on your validation data?\n",
        "\n",
        "> **3. Determine your evaluation protocol:** hold-out validation? K-fold validation? Which portion of the data should you use for validation?\n",
        "\n",
        "> **4. Develop a first model that does better than a basic baseline:** a model with statistical power.\n",
        "\n",
        "> **5. Develop a model that overfits.** The universal tension in machine learning is between optimization and generalization; the ideal model is one that stands right at the border between underfitting and overfitting; between undercapacity and overcapacity. To figure out where this border lies, first you must cross it.\n",
        "\n",
        "> **6. Regularize your model and tune its hyperparameters, based on performance on the validation data.** Repeatedly modify your model, train it, evaluate on your validation data (not the test data, at this point), modify it again, and repeat, until the model is as good as it can get. \n",
        "\n",
        "> **Iterate on feature engineering: add new features, or remove features that don’t seem to be informative.** Once you’ve developed a satisfactory model configuration, you can train your final production model on all the available data (training and validation) and evaluate it one last time on the test set.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVkF-0LCkPf3",
        "colab_type": "text"
      },
      "source": [
        "## 1. Define the problem at hand and the data on which you'll train\n",
        "\n",
        "This isn't easy! You have to define your target, join tables, and avoid leakage. This opinionated blog post explains:\n",
        "\n",
        "#### [Data Science Is Not Taught At Universities - And Here Is Why](https://www.linkedin.com/pulse/data-science-taught-universities-here-why-maciej-wasiak/)\n",
        "\n",
        "> The tables they use in machine learning research already have the target information clearly defined. Here comes the famous IRIS dataset, then the Wisconsin Breast Cancer, there is even Credit Risk or Telco Churn data and they all have the **Target** column there ...\n",
        "\n",
        "> The problem is that in real life the **Target** flag is NEVER there.\n",
        "\n",
        "> For churn modelling you may have many churn types on the system and need to distil the few that need modelling. And hey - when a subscriber moves from Postpaid contract to Prepaid – is this a churn or not? (‘Yes’ – says the Postpaid Base Manager, ‘No’ says the CEO ). You have to make the call ...\n",
        "\n",
        "> Your source will be a database with tens or hundreds of **tables**, millions of records, usually after 3 painful migrations with gaps in history, columns without descriptions ...\n",
        "\n",
        "> Flooded by **leaks from the future**, ...a dozen of other traps ... And you need to disarm all of them, because even one left behind may result in a completely useless model. \n",
        "\n",
        "> These are the skills employers are looking for."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x10K2KZakUNW",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "## Regression or Classification?\n",
        "\n",
        "#### You can convert problems from regression to classification\n",
        "\n",
        "1. UCI, [Adult Census Income dataset](https://archive.ics.uci.edu/ml/datasets/adult)\n",
        "\n",
        "2. DS5 student Han Lee, [Bitcoin Price Prediction app](https://dry-shore-97069.herokuapp.com/about):\n",
        "\n",
        "> We also cared a lot more about the direction of returns instead of magnitude of returns. A trade placed based on the prediction that the price to go up tomorrow will be fine if the magnitude is off but will be unprofitable if the direction is wrong. ... Yesterday's return is unsurprising a great predictor for today's return, but has a poor directional accuracy.\n",
        "\n",
        "#### You can convert problems from classification to regression\n",
        "\n",
        "Brandon Rohrer, [What questions can machine learning answer](https://brohrer.github.io/five_questions_data_science_answers.html)\n",
        "\n",
        "> Sometimes questions that look like multi-value classification questions are actually better suited to regression. For instance, “Which news story is the most interesting to this reader?” appears to ask for a category—a single item from the list of news stories. However, you can reformulate it to “How interesting is each story on this list to this reader?” and give each article a numerical score. Then it is a simple thing to identify the highest-scoring article. Questions of this type often occur as rankings or comparisons.\n",
        "\n",
        "> “Which van in my fleet needs servicing the most?” can be rephrased as “How badly does each van in my fleet need servicing?” \n",
        "“Which 5% of my customers will leave my business for a competitor in the next year?” can be rephrased as “How likely is each of my customers to leave my business for a competitor in the next year?” \n",
        "\n",
        "> Binary classification problems can also be reformulated as regression. (In fact, under the hood some algorithms reformulate every binary classification as regression.) This is especially helpful when an example can belong part A and part B, or have a chance of going either way. When an answer can be partly yes and no, probably on but possibly off, then regression can reflect that. Questions of this type often begin “How likely…” or “What fraction…”\n",
        "\n",
        "> How likely is this user to click on my ad? What fraction of pulls on this slot machine result in payout? How likely is this employee to be an insider security threat? What fraction of today’s flights will depart on time?\n",
        "\n",
        "We'll see examples of this, using predicted probabilities instead of discrete predictions, with Tanzania Waterpumps and Lending Club data.\n",
        "\n",
        "#### You can convert multi-class classification to binary classification\n",
        "\n",
        "By omitting or combining some classes. We'll also see examples of this, with Tanzania Waterpumps and Lending Club data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPI_ZfkdPsdv",
        "colab_type": "text"
      },
      "source": [
        "## Lending Club example 🏦\n",
        "\n",
        "### Background\n",
        "\n",
        "[According to Wikipedia,](https://en.wikipedia.org/wiki/Lending_Club)\n",
        "\n",
        "> Lending Club is the world's largest peer-to-peer lending platform. Lending Club enables borrowers to create unsecured personal loans between \\$1,000 and \\$40,000. The standard loan period is three years. Investors can search and browse the loan listings on Lending Club website and select loans that they want to invest in based on the information supplied about the borrower, amount of loan, loan grade, and loan purpose. Investors make money from interest. Lending Club makes money by charging borrowers an origination fee and investors a service fee.\n",
        "\n",
        "[Lending Club says,](https://www.lendingclub.com/) \"Our mission is to transform the banking system to make credit more affordable and investing more rewarding.\" You can view their [loan statistics and visualizations](https://www.lendingclub.com/info/demand-and-credit-profile.action).\n",
        "\n",
        "Lending Club's [Investor Education Center](https://www.lendingclub.com/investing/investor-education) can help you grow your domain expertise. The article about [Benefits of diversification](https://www.lendingclub.com/investing/investor-education/benefits-of-diversification) explains,\n",
        "\n",
        "> With the investment minimum of \\$1,000, you can get up to 40 Notes at \\$25 each.\n",
        "\n",
        "![](https://i.ibb.co/B37q8LB/www-lendingclub-com-browse-browse-action-1.png)\n",
        "\n",
        "### Data sources\n",
        "- [Current loans](https://www.lendingclub.com/browse/browse.action)\n",
        "- [Data Dictionary & Historical loans](https://www.lendingclub.com/info/download-data.action) (17 zip files, 450 MB total)\n",
        "\n",
        "### What questions could we ask with this data?\n",
        "1. Can we predict the interest rate that Lending Club will assign to a loan, to reverse engineer their formula. (Regression problem. Can only use info from before the interest rate was assigned)\n",
        "2. Can we predict whether a loan will be fully paid or charged off, to choose which loans to invest in. (Classification problem. Can only use info available at the time you choose loans, from loans that have been fully paid or charged off.)\n",
        "\n",
        "[Here's a Plotly Dash app for #1](https://rrherr-project2-example.herokuapp.com/).\n",
        "\n",
        "This notebook will work on #2.\n",
        "\n",
        "\n",
        "### Use a subset of Loan Status\n",
        "\n",
        "#### [Data-Driven Investment Strategies for Peer-to-Peer Lending: A Case Study for Teaching Data Science](https://www.liebertpub.com/doi/full/10.1089/big.2018.0092)\n",
        "\n",
        "> Current refers to a loan that is still being reimbursed in a timely manner. Late corresponds to a loan on which a payment is between 16 and 120 days overdue. If the payment is delayed by more than 121 days, the loan is considered to be in Default. If LendingClub has decided that the loan will not be paid off, then it is given the status of Charged-Off.\n",
        "\n",
        "> These dynamics imply that 5 months after the term of each loan has ended, every loan ends in one of two LendingClub states—fully paid or charged-off. We call these two statuses fully paid and defaulted, respectively, and we refer to a loan that has reached one of these statuses as expired.\n",
        "\n",
        "> **One way to simplify the problem is to consider only loans that have expired at the time of analysis.**\n",
        "\n",
        "> A significant portion (13.5%) of loans ended in Default status; depending on how much of the loan was paid back, these loans\n",
        "might have resulted in a significant loss to investors who had invested in them. The remainder was Fully Paid—the borrower fully reimbursed the loan’s outstanding balance with interest, and the investor earned a positive return on his or her investment. Therefore, to avoid unsuccessful investments, our goal is to estimate which loans are more likely to default and which will yield low returns. \n",
        "\n",
        "### Use a subset of Loan Grade\n",
        "\n",
        "[Lending Club announced,](https://blog.lendingclub.com/q1-2019-platform-update) \n",
        "\n",
        "> We periodically adjust platform products to reflect changes in investor demand and other marketplace factors. As a result, this quarter we are retiring Grade E loans. As of May 7, 2019, we will no longer facilitate new Grade E loans except for certain previously qualified or approved loans; **effective July 1, 2019, no grade E loans will be available on the platform.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcfo7iw2hXVh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "pd.options.display.max_columns = 200\n",
        "pd.options.display.max_rows = 200\n",
        "\n",
        "history_location = '../data/lending-club/lending-club-subset.csv'\n",
        "current_location = '../data/lending-club/primaryMarketNotes_browseNotes_1-RETAIL.csv'\n",
        "\n",
        "# Stratified sample, 10% of expired Lending Club loans, grades A-D\n",
        "# Source: https://www.lendingclub.com/info/download-data.action\n",
        "history = pd.read_csv(history_location)\n",
        "history['issue_d'] = pd.to_datetime(history['issue_d'], infer_datetime_format=True)\n",
        "\n",
        "# Current loans available for manual investing, June 17, 2019\n",
        "# Source: https://www.lendingclub.com/browse/browse.action\n",
        "current = pd.read_csv(current_location)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82fTVpekhk5h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate percent of each loan repaid\n",
        "history['percent_paid'] = history['total_pymnt'] / history['funded_amnt']\n",
        "\n",
        "# See percent paid for charged off vs fully paid loans\n",
        "history.groupby('loan_status')['percent_paid'].describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XjgjkoMhq2n",
        "colab_type": "text"
      },
      "source": [
        "### Begin with baselines: expected value of random decisions\n",
        "\n",
        "[You May Be Better Off Picking Stocks at Random, Study Finds](https://news.ycombinator.com/item?id=20724338)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHtTfKQCh3AS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import percentileofscore\n",
        "import seaborn as sns\n",
        "from tqdm import tnrange\n",
        "\n",
        "def simulate(n=10000, grades=['A','B','C','D'], \n",
        "             start_date='2007-07-01', \n",
        "             end_date='2019-03-01'):\n",
        "    \"\"\"\n",
        "    What if you picked 40 random loans for $25 investments?\n",
        "    How much would you have been paid back?\n",
        "    \n",
        "    Repeat the simulation many times, and plot the distribution \n",
        "    of probable outcomes.\n",
        "    \n",
        "    This doesn't consider fees or \"time value of money.\"\n",
        "    \"\"\"\n",
        "    \n",
        "    condition = ((history['grade'].isin(grades)) & \n",
        "                 (history['issue_d'] >= start_date) &\n",
        "                 (history['issue_d'] <= end_date))\n",
        "    possible = history[condition]\n",
        "    \n",
        "    simulations = []\n",
        "    for _ in tnrange(n):\n",
        "        picks = possible.sample(40).copy()\n",
        "        picks['paid'] = 25 * picks['percent_paid']\n",
        "        paid = picks['paid'].sum()\n",
        "        simulations.append(paid)\n",
        "        \n",
        "    simulations = pd.Series(simulations)\n",
        "    sns.distplot(simulations)\n",
        "    plt.axvline(x=1000)\n",
        "    percent = percentileofscore(simulations, 1000)\n",
        "    plt.title(f'{percent}% of simulations did not profit. {start_date}-{end_date}, {grades}')\n",
        "\n",
        "simulate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2AYFbFph7FY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "simulate(grades=['A'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCupcdT8h79y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "simulate(grades=['D'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV9G_udvhTUp",
        "colab_type": "text"
      },
      "source": [
        "### Use a subset of features\n",
        "\n",
        "What subset of features should we use, to avoid leakage?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Mf-PiPTiDw6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsmRpA2jgFc_",
        "colab_type": "text"
      },
      "source": [
        "## 2. Choose your evaluation metric: how you’ll measure success on your problem\n",
        "\n",
        "What are some of your [options](https://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values)? Let's go back to Tanzania Waterpumps to learn about some new options."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCcoRbHMgRkB",
        "colab_type": "text"
      },
      "source": [
        "## Tanzania Waterpumps example🚰"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_h39JycQu8J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Merge train_features.csv & train_labels.csv\n",
        "train = pd.merge(pd.read_csv('../data/tanzania/train_features.csv'), \n",
        "                 pd.read_csv('../data/tanzania/train_labels.csv'))\n",
        "\n",
        "# Read test_features.csv & sample_submission.csv\n",
        "test = pd.read_csv('../data/tanzania/test_features.csv')\n",
        "sample_submission = pd.read_csv('../data/tanzania/sample_submission.csv')\n",
        "\n",
        "\n",
        "# Split train into train & val. Make val the same size as test.\n",
        "train, val = train_test_split(train, test_size=len(test),  \n",
        "                              stratify=train['status_group'], random_state=42)\n",
        "\n",
        "def wrangle(X):\n",
        "    \"\"\"Wrangle train, validate, and test sets in the same way\"\"\"\n",
        "    \n",
        "    # Prevent SettingWithCopyWarning\n",
        "    X = X.copy()\n",
        "    \n",
        "    # About 3% of the time, latitude has small values near zero,\n",
        "    # outside Tanzania, so we'll treat these values like zero.\n",
        "    X['latitude'] = X['latitude'].replace(-2e-08, 0)\n",
        "    \n",
        "    # When columns have zeros and shouldn't, they are like null values.\n",
        "    # So we will replace the zeros with nulls, and impute missing values later.\n",
        "    # Also create a \"missing indicator\" column, because the fact that\n",
        "    # values are missing may be a predictive signal.\n",
        "    cols_with_zeros = ['longitude', 'latitude', 'construction_year', \n",
        "                       'gps_height', 'population']\n",
        "    for col in cols_with_zeros:\n",
        "        X[col] = X[col].replace(0, np.nan)\n",
        "        X[col+'_MISSING'] = X[col].isnull()\n",
        "            \n",
        "    # Drop duplicate columns\n",
        "    duplicates = ['quantity_group', 'payment_type']\n",
        "    X = X.drop(columns=duplicates)\n",
        "    \n",
        "    # Drop recorded_by (never varies) and id (always varies, random)\n",
        "    unusable_variance = ['recorded_by', 'id']\n",
        "    X = X.drop(columns=unusable_variance)\n",
        "    \n",
        "    # Convert date_recorded to datetime\n",
        "    X['date_recorded'] = pd.to_datetime(X['date_recorded'], infer_datetime_format=True)\n",
        "    \n",
        "    # Extract components from date_recorded, then drop the original column\n",
        "    X['year_recorded'] = X['date_recorded'].dt.year\n",
        "    X['month_recorded'] = X['date_recorded'].dt.month\n",
        "    X['day_recorded'] = X['date_recorded'].dt.day\n",
        "    X = X.drop(columns='date_recorded')\n",
        "    \n",
        "    # Engineer feature: how many years from construction_year to date_recorded\n",
        "    X['years'] = X['year_recorded'] - X['construction_year']\n",
        "    X['years_MISSING'] = X['years'].isnull()\n",
        "    \n",
        "    # return the wrangled dataframe\n",
        "    return X\n",
        "\n",
        "train = wrangle(train)\n",
        "val = wrangle(val)\n",
        "test = wrangle(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ay-MPeZuQv51",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Arrange data into X features matrix and y target vector\n",
        "target = 'status_group'\n",
        "X_train = train.drop(columns=target)\n",
        "y_train = train[target]\n",
        "X_val = val.drop(columns=target)\n",
        "y_val = val[target]\n",
        "X_test = test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gdJqQhVQ1SZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import category_encoders as ce\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "pipeline = make_pipeline(\n",
        "    ce.OrdinalEncoder(), \n",
        "    SimpleImputer(strategy='median'), \n",
        "    RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        ")\n",
        "\n",
        "# Fit on train, score on val\n",
        "pipeline.fit(X_train, y_train)\n",
        "print('Validation Accuracy', pipeline.score(X_val, y_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfhziD2Wn_iO",
        "colab_type": "text"
      },
      "source": [
        "## Get and interpret the confusion matrix for classification models\n",
        "\n",
        "[Scikit-Learn User Guide — Confusion Matrix](https://scikit-learn.org/stable/modules/model_evaluation.html#confusion-matrix)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MSWehj9n_iO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fP6FGBGUn_iQ",
        "colab_type": "text"
      },
      "source": [
        "#### How many correct predictions were made?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRSaYRPWn_iR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-q-3R7Ean_iT",
        "colab_type": "text"
      },
      "source": [
        "#### How many total predictions were made?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLAQL05fn_iT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1yQ_jYPn_iV",
        "colab_type": "text"
      },
      "source": [
        "#### What was the classification accuracy?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fskAC6SYn_iW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqFgEm3tn_iY",
        "colab_type": "text"
      },
      "source": [
        "## Use classification metrics: precision, recall\n",
        "[Scikit-Learn User Guide — Classification Report](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-report)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGv7OLL4n_iY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1U7HdC6n_ia",
        "colab_type": "text"
      },
      "source": [
        "#### Wikipedia, [Precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall)\n",
        "\n",
        "> Both precision and recall are based on an understanding and measure of relevance.\n",
        "\n",
        "> Suppose a computer program for recognizing dogs in photographs identifies 8 dogs in a picture containing 12 dogs and some cats. Of the 8 identified as dogs, 5 actually are dogs (true positives), while the rest are cats (false positives). The program's precision is 5/8 while its recall is 5/12.\n",
        "\n",
        "> High precision means that an algorithm returned substantially more relevant results than irrelevant ones, while high recall means that an algorithm returned most of the relevant results.\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/700px-Precisionrecall.svg.png\" width=\"400\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50R-Xhwdn_ie",
        "colab_type": "text"
      },
      "source": [
        "#### [We can get precision & recall from the confusion matrix](https://en.wikipedia.org/wiki/Precision_and_recall#Definition_(classification_context))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIta6Vwsn_if",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iY2rfzA4n_ih",
        "colab_type": "text"
      },
      "source": [
        "#### How many correct predictions of \"non functional\"?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-anLkCin_ii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYM6f99cn_ij",
        "colab_type": "text"
      },
      "source": [
        "#### How many total predictions of \"non functional\"?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qCiA8j2n_ik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXNuZ_Rnn_il",
        "colab_type": "text"
      },
      "source": [
        "#### What's the precision for \"non functional\"?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1f7VsyXn_im",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci4QguAkn_in",
        "colab_type": "text"
      },
      "source": [
        "#### How many actual \"non functional\" waterpumps?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlqxNhlYn_io",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IY-vC-hn_iq",
        "colab_type": "text"
      },
      "source": [
        "#### What's the recall for \"non functional\"?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ukch-6Zn_iq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObVED_ugn_is",
        "colab_type": "text"
      },
      "source": [
        "## Understand the relationships between precision, recall, thresholds, and predicted probabilities. Understand how Precision@K can help make decisions and allocate budgets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBcQQJ2kn_is",
        "colab_type": "text"
      },
      "source": [
        "### Imagine this scenario...\n",
        "\n",
        "Suppose there are over 14,000 waterpumps that you _do_ have some information about, but you _don't_ know whether they are currently functional, or functional but need repair, or non-functional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEEy86CHn_it",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3az2llAAn_iu",
        "colab_type": "text"
      },
      "source": [
        "**You have the time and resources to go to just 2,000 waterpumps for proactive maintenance.** You want to predict, which 2,000 are most likely non-functional or in need of repair, to help you triage and prioritize your waterpump inspections.\n",
        "\n",
        "You have historical inspection data for over 59,000 other waterpumps, which you'll use to fit your predictive model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEWc2zt2n_iv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(train) + len(val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2LiGJLin_ix",
        "colab_type": "text"
      },
      "source": [
        "You have historical inspection data for over 59,000 other waterpumps, which you'll use to fit your predictive model.\n",
        "\n",
        "Based on this historical data, if you randomly chose waterpumps to inspect, then about 46% of the waterpumps would need repairs, and 54% would not need repairs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JliDXTp5n_iy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train.value_counts(normalize=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLnJ7Fnan_i1",
        "colab_type": "text"
      },
      "source": [
        "**Can you do better than random at prioritizing inspections?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIh2Xj8fn_i3",
        "colab_type": "text"
      },
      "source": [
        "In this scenario, we should define our target differently. We want to identify which waterpumps are non-functional _or_ are functional but needs repair:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7naqusI0n_i4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = y_train != 'functional'\n",
        "y_val = y_val != 'functional'\n",
        "y_train.value_counts(normalize=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1UR1t8Zn_i6",
        "colab_type": "text"
      },
      "source": [
        "We already made our validation set the same size as our test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHHIplB7n_i8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(val) == len(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g41DA70rn_i9",
        "colab_type": "text"
      },
      "source": [
        "We can refit our model, using the redefined target.\n",
        "\n",
        "Then make predictions for the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXL0LaXQn_i-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pipeline.fit(X_train, y_train)\n",
        "y_pred = pipeline.predict(X_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qISPzM43n_jA",
        "colab_type": "text"
      },
      "source": [
        "And look at the confusion matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y72fakpmn_jB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_confusion_matrix(y_val, y_pred);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M30BXR6Rn_jC",
        "colab_type": "text"
      },
      "source": [
        "#### How many total predictions of \"True\" (\"non functional\" or \"functional needs repair\") ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IeTJFo8n_jD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "5032 + 977"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aZSdskSn_jF",
        "colab_type": "text"
      },
      "source": [
        "#### We don't have \"budget\" to take action on all these predictions\n",
        "\n",
        "- But we can get predicted probabilities, to rank the predictions. \n",
        "- Then change the threshold, to change the number of positive predictions, based on our budget."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXkfXDDZn_jF",
        "colab_type": "text"
      },
      "source": [
        "### Get predicted probabilities and plot the distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwfe7j7W_jTp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eD6pRFKOn_jH",
        "colab_type": "text"
      },
      "source": [
        "### Change the threshold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjOhH0BMB55A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "top80m_Gn_jI",
        "colab_type": "text"
      },
      "source": [
        "### In this scenario ... \n",
        "\n",
        "Accuracy _isn't_ the best metric!\n",
        "\n",
        "Instead, change the threshold, to change the number of positive predictions, based on the budget. (You have the time and resources to go to just 2,000 waterpumps for proactive maintenance.)\n",
        "\n",
        "Then, evaluate with the precision for \"non functional\"/\"functional needs repair\".\n",
        "\n",
        "This is conceptually like **Precision@K**, where k=2,000.\n",
        "\n",
        "Read more here: [Recall and Precision at k for Recommender Systems: Detailed Explanation with examples](https://medium.com/@m_n_malaeb/recall-and-precision-at-k-for-recommender-systems-618483226c54)\n",
        "\n",
        "> Precision at k is the proportion of recommended items in the top-k set that are relevant\n",
        "\n",
        "> Mathematically precision@k is defined as: `Precision@k = (# of recommended items @k that are relevant) / (# of recommended items @k)`\n",
        "\n",
        "> In the context of recommendation systems we are most likely interested in recommending top-N items to the user. So it makes more sense to compute precision and recall metrics in the first N items instead of all the items. Thus the notion of precision and recall at k where k is a user definable integer that is set by the user to match the top-N recommendations objective.\n",
        "\n",
        "We asked, can you do better than random at prioritizing inspections?\n",
        "\n",
        "If we had randomly chosen waterpumps to inspect, we estimate that only 920 waterpumps would be repaired after 2,000 maintenance visits. (46%)\n",
        "\n",
        "But using our predictive model, in the validation set, we succesfully identified over 1,600 waterpumps in need of repair!\n",
        "\n",
        "So we will use this predictive model with the dataset of over 14,000 waterpumps that we _do_ have some information about, but we _don't_ know whether they are currently functional, or functional but need repair, or non-functional.\n",
        "\n",
        "We will predict which 2,000 are most likely non-functional or in need of repair.\n",
        "\n",
        "We estimate that approximately 1,600 waterpumps will be repaired after these 2,000 maintenance visits.\n",
        "\n",
        "So we're confident that our predictive model will help triage and prioritize waterpump inspections."
      ]
    }
  ]
}